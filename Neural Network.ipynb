{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "574d8358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from scipy.stats import pearsonr\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68aee529",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH: str = \"./School_Absenteeism_Cleaned.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9e18652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEFINING INDEXES INTO THE X AND Y DATASETS\n",
    "X_FEMALE_PERCENTAGE = 0\n",
    "X_MALE_PERCENTAGE = 1\n",
    "X_ASIAN_PERCENTAGE = 2\n",
    "X_BLACK_PERCENTAGE = 3\n",
    "X_HISPANIC_PERCENTAGE = 4\n",
    "X_MULTIRACIAL_PERCENTAGE = 5\n",
    "X_NATIVE_AMERICAN_PERCENTAGE = 6\n",
    "X_WHITE_PERCENTAGE = 7\n",
    "#X_MISSING_RACE_PERCENTAGE = 8\n",
    "X_DISABILITIES_PERCENTAGE = 8\n",
    "X_ENGLISH_LANGUAGE_LEARNERS_PERCENTAGE = 9\n",
    "X_POVERTY_PERCENTAGE = 10\n",
    "X_ECONOMIC_PERCENTAGE = 11\n",
    "\n",
    "Y_ATTENDANCE_PERCENTAGE = 0\n",
    "Y_CHRONICALLY_ABSENT_PERCENTAGE = 1\n",
    "Y_MEAN_SCALE_SCORE_E = 2\n",
    "Y_MEAN_SCALE_SCORE_M = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6deab2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts the dataset from the csv into a numpy.ndarray (this is just a fancy array)\n",
    "# This type of array is just easy to use for pytorch and other stats libraries\n",
    "def extract_dataset(dataset_path: str) -> tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    X_list: list[list] = []\n",
    "    Y_list: list[list] = []\n",
    "    with open(dataset_path) as dataaset_file:\n",
    "        \n",
    "        csvreader = csv.reader(dataaset_file)\n",
    "        next(csvreader)\n",
    "        \n",
    "        \n",
    "        for line in csvreader:\n",
    "            index = line[0]\n",
    "            dbn = line[1] \n",
    "            year = line[2] \n",
    "            mean_scale_score_e = float(line[3])\n",
    "            mean_scale_score_m = float(line[4]) \n",
    "            total_enrolement = line[5]\n",
    "            female_percentage = float(line[6])\n",
    "            male_percentage = float(line[7])\n",
    "            asian_percentage = float(line[8])\n",
    "            black_percentage = float(line[9]) \n",
    "            hispanic_percentage = float(line[10])\n",
    "            multiracial_percentage = float(line[11])\n",
    "            native_american_percentage = float(line[12]) \n",
    "            white_percentage = float(line[13]) \n",
    "            missing_race_data_percentage = float(line[14])\n",
    "            disabilities_percentage = float(line[15])\n",
    "            english_language_learners = float(line[16])\n",
    "            poverty_percentage = float(line[17])\n",
    "            economic_need_index = float(line[18])\n",
    "            num_total_days_a = line[19] \n",
    "            num_days_absent_a = line[20]\n",
    "            num_days_present_a = line[21] \n",
    "            attendance_percentage_a = float(line[22]) / 100\n",
    "            num_contributing_total_pres_day_a = line[23] \n",
    "            num_chronically_absent_a = line[24] \n",
    "            chronically_absent_percentage_a = float(line[25]) / 100\n",
    "            tested_percentage_e = float(line[26]) \n",
    "            tested_percentage_m = float(line[27])\n",
    "            \n",
    "            x_row: list[float] = [\n",
    "                female_percentage,\n",
    "                male_percentage,\n",
    "                asian_percentage,\n",
    "                black_percentage,\n",
    "                hispanic_percentage,\n",
    "                multiracial_percentage,\n",
    "                native_american_percentage,\n",
    "                white_percentage, \n",
    "                disabilities_percentage,\n",
    "                english_language_learners,\n",
    "                poverty_percentage,\n",
    "                economic_need_index\n",
    "            ]\n",
    "            \n",
    "            y_row: list[float] = [\n",
    "                attendance_percentage_a,\n",
    "                chronically_absent_percentage_a,\n",
    "                mean_scale_score_e,\n",
    "                mean_scale_score_m\n",
    "            ]\n",
    "            \n",
    "            \n",
    "            X_list.append(x_row)\n",
    "            Y_list.append(y_row)\n",
    "        \n",
    "            \n",
    "    return np.array(X_list), np.array(Y_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ef43518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear correlation between asian percentage and attendance is 0.4693007420084773\n",
      "[[ 0.03000492 -0.02874846  0.01736541  0.01361121]\n",
      " [-0.0299081   0.02871256 -0.01700019 -0.01323784]\n",
      " [ 0.46930074 -0.46509995  0.0658102   0.0962609 ]\n",
      " [-0.45525613  0.45025482 -0.02634468 -0.05276537]\n",
      " [-0.2017007   0.24215728 -0.08679253 -0.09952265]\n",
      " [ 0.2443545  -0.28569156  0.01417946  0.02678091]\n",
      " [-0.0862014   0.10258708 -0.02175912 -0.0238714 ]\n",
      " [ 0.40298859 -0.45029748  0.09743478  0.11886302]\n",
      " [-0.44366626  0.43747134 -0.03538222 -0.05660722]\n",
      " [ 0.05190869 -0.00126451 -0.0965947  -0.08736029]\n",
      " [-0.47794021  0.53856617 -0.1176475  -0.138295  ]\n",
      " [-0.52488779  0.58279953 -0.13803483 -0.16007572]]\n",
      "Epoch 0, Train Loss: 0.31733566522598267\n",
      "Epoch 100, Train Loss: 0.1572992503643036\n",
      "Epoch 200, Train Loss: 0.1522122323513031\n",
      "Epoch 300, Train Loss: 0.15040172636508942\n",
      "Epoch 400, Train Loss: 0.1494378298521042\n",
      "Epoch 500, Train Loss: 0.1487985998392105\n",
      "Epoch 600, Train Loss: 0.14832232892513275\n",
      "Epoch 700, Train Loss: 0.14793266355991364\n",
      "Epoch 800, Train Loss: 0.14764003455638885\n",
      "Epoch 900, Train Loss: 0.14741484820842743\n",
      "Epoch 1000, Train Loss: 0.1472376137971878\n",
      "Epoch 1100, Train Loss: 0.14709503948688507\n",
      "Epoch 1200, Train Loss: 0.1469791829586029\n",
      "Epoch 1300, Train Loss: 0.14688704907894135\n",
      "Epoch 1400, Train Loss: 0.14681583642959595\n",
      "Epoch 1500, Train Loss: 0.1467563658952713\n",
      "Epoch 1600, Train Loss: 0.14670409262180328\n",
      "Epoch 1700, Train Loss: 0.14666396379470825\n",
      "Epoch 1800, Train Loss: 0.1466306746006012\n",
      "Epoch 1900, Train Loss: 0.14660309255123138\n",
      "Epoch 2000, Train Loss: 0.14657822251319885\n",
      "Epoch 2100, Train Loss: 0.14655590057373047\n",
      "Epoch 2200, Train Loss: 0.14653736352920532\n",
      "Epoch 2300, Train Loss: 0.14651980996131897\n",
      "Epoch 2400, Train Loss: 0.14650505781173706\n",
      "Epoch 2500, Train Loss: 0.14649271965026855\n",
      "Epoch 2600, Train Loss: 0.14648258686065674\n",
      "Epoch 2700, Train Loss: 0.14647354185581207\n",
      "Epoch 2800, Train Loss: 0.1464654803276062\n",
      "Epoch 2900, Train Loss: 0.14645837247371674\n",
      "Epoch 3000, Train Loss: 0.14645209908485413\n",
      "Epoch 3100, Train Loss: 0.1464458703994751\n",
      "Epoch 3200, Train Loss: 0.14644008874893188\n",
      "Epoch 3300, Train Loss: 0.14643464982509613\n",
      "Epoch 3400, Train Loss: 0.14642882347106934\n",
      "Epoch 3500, Train Loss: 0.1464230865240097\n",
      "Epoch 3600, Train Loss: 0.14641740918159485\n",
      "Epoch 3700, Train Loss: 0.14641132950782776\n",
      "Epoch 3800, Train Loss: 0.1464054137468338\n",
      "Epoch 3900, Train Loss: 0.14639894664287567\n",
      "Epoch 4000, Train Loss: 0.14639241993427277\n",
      "Epoch 4100, Train Loss: 0.14638520777225494\n",
      "Epoch 4200, Train Loss: 0.1463780403137207\n",
      "Epoch 4300, Train Loss: 0.1463708132505417\n",
      "Epoch 4400, Train Loss: 0.14636288583278656\n",
      "Epoch 4500, Train Loss: 0.14635515213012695\n",
      "Epoch 4600, Train Loss: 0.14634743332862854\n",
      "Epoch 4700, Train Loss: 0.14633871614933014\n",
      "Epoch 4800, Train Loss: 0.1463300585746765\n",
      "Epoch 4900, Train Loss: 0.14632122218608856\n",
      "=====================\n",
      "Validation Loss: 0.15155984461307526\n",
      "=====================\n",
      "Weights of the model \n",
      "==============\n",
      "You can interpret this as how much they contribute to each of the Y features, the indexes of the weights corresponds to each of the X features in order, so female percentage, male percentage, asian percentage, black percentage, ... etc are the first index, second index, ...\n",
      "\n",
      "Attendance Weights: \n",
      "tensor([ 0.9942,  1.2001,  1.6885,  0.9230,  1.2785, -1.9402, -0.9272,  0.9352,\n",
      "        -0.5909,  0.0878,  0.1204, -1.3391])\n",
      "Chronically Absent Weights: \n",
      "tensor([-0.8232, -1.1229, -1.4071, -0.2247, -0.8149,  5.4163,  3.7141, -0.2246,\n",
      "         1.1131, -0.1324, -0.1642,  2.6137])\n",
      "ELA Test Scores: \n",
      "tensor([ 2.2100e-01, -2.5276e-01,  3.0046e+00,  2.3062e+00,  2.5033e+00,\n",
      "        -2.3344e+00,  1.1404e+00,  2.5097e+00,  1.2465e-03, -6.4549e-01,\n",
      "        -8.0624e-02, -9.9773e-01])\n",
      "Math Test Scores: \n",
      "tensor([-0.2423, -0.3120,  3.3460,  2.5608,  2.7978, -3.4622,  1.1597,  2.8130,\n",
      "        -0.0614, -0.4977, -0.2106, -0.9525])\n",
      "=====================\n",
      "To interpret this, a positive weight for an X feature means that this feature contributes positively to that Y feature. So for my training, Asian Percentage (the third value), had a positive weight for attendance which means that the model learned that a large Asian percentage contributes positively to a larger attendance (this technically isn't linear so we can't say the model learned a linear correlation)\n",
      "=====================\n"
     ]
    }
   ],
   "source": [
    "# This calculates pearson (linear) correlation coefficient for each \n",
    "# of the X features against each of the y features\n",
    "def pearson_correlation(X: np.ndarray, Y: np.ndarray) -> np.ndarray:\n",
    "    # Assuming X and Y are your datasets\n",
    "    num_cols_X = X.shape[1]\n",
    "    num_cols_Y = Y.shape[1]\n",
    "\n",
    "    # Initialize a matrix to hold the Pearson correlation coefficients\n",
    "    pearson_correlation_matrix = np.zeros((num_cols_X, num_cols_Y))\n",
    "\n",
    "    # Calculate the Pearson correlation coefficient for each column in X against\n",
    "    # each column in Y\n",
    "    for i in range(num_cols_X):\n",
    "        for j in range(num_cols_Y):\n",
    "            # Compute the Pearson correlation coefficient\n",
    "            correlation, _ = pearsonr(X[:, i], Y[:, j])\n",
    "            pearson_correlation_matrix[i, j] = correlation\n",
    "    \n",
    "    return pearson_correlation_matrix\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X: np.ndarray\n",
    "    Y: np.ndarray\n",
    "    \n",
    "    X, Y = extract_dataset(DATASET_PATH)\n",
    "    \n",
    "    correlation_coeffs: np.ndarray = pearson_correlation(X, Y)\n",
    "    \n",
    "    # An example of how to get the correlation coefficients from the correlation coeffs matrix\n",
    "    #print(f\"Linear correlation between asian percentage and attendance is {correlation_coeffs[X_ASIAN_PERCENTAGE, Y_MEAN_SCALE_SCORE_E]}\")\n",
    "    print(f\"Linear correlation between asian percentage and attendance is {correlation_coeffs[X_ASIAN_PERCENTAGE, Y_ATTENDANCE_PERCENTAGE]}\")\n",
    "    print(correlation_coeffs)\n",
    "    \n",
    "    #plt.scatter(X[:,X_ASIAN_PERCENTAGE], Y[:, Y_ATTENDANCE_PERCENTAGE])\n",
    "    #plt.xlabel(\"Asian Percentage\")\n",
    "    #plt.ylabel(\"Attendance\")\n",
    "    #plt.show()\n",
    "    \n",
    "    \n",
    "    X_train, X_val, Y_train, Y_val = train_test_split( # split the dataset into a train and test split\n",
    "        X, \n",
    "        Y, \n",
    "        test_size = 0.2, \n",
    "        random_state = 42\n",
    "    )\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32) # convert to tensors (similar to numpy.ndarray but for pytorch)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "    Y_train = torch.tensor(Y_train, dtype=torch.float32)\n",
    "    Y_val = torch.tensor(Y_val, dtype=torch.float32)\n",
    "    \n",
    "    \n",
    "    model: nn.Sequential = nn.Sequential( # Seqiential models just pass data through each of the following modules\n",
    "        nn.Linear(X.shape[1], Y.shape[1]), # (fully connected layer that learns linear relationships between X and y)\n",
    "        nn.Sigmoid() # Maps output to range (0, 1) since we are predicting values in this range\n",
    "    )\n",
    "    \n",
    "    loss_fn: nn.Module = nn.L1Loss() # This loss is just the difference between prediction and true value e.g L1Loss(5, 9) = 4\n",
    "    optimiser: torch.optim.Optimizer = torch.optim.Adam(model.parameters(), lr=0.01) # Controls how the weights are updated\n",
    "    \n",
    "    # Number of epochs (how many times we are training on the same dataset)\n",
    "    epochs = 5000\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Set model to training mode\n",
    "        model.train()\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimiser.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        Y_pred = model(X_train)\n",
    "\n",
    "        # Compute loss between model prediction and the true value\n",
    "        loss: nn.Module = loss_fn(Y_pred, Y_train)\n",
    "        \n",
    "        # Backward pass (calculates what updates need to be made to the model)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights of the model\n",
    "        optimiser.step()\n",
    "\n",
    "        # Print loss every 100 epochs\n",
    "        if epoch % 100 == 0:\n",
    "            print(f'Epoch {epoch}, Train Loss: {loss.item()}')\n",
    "\n",
    "    # Evaluate the model on validation dataset\n",
    "    model.eval()\n",
    "    # torch.no_grad is just saying we don't need to track values for backpropagation (weight updating)\n",
    "    with torch.no_grad():\n",
    "        Y_val_pred = model(X_val) # pass the X validation dataset into the model to get the predictions for this dataset\n",
    "        val_loss = loss_fn(Y_val_pred, Y_val) # calculate the loss of the validation dataset (ou want this close to the training loss)\n",
    "    \n",
    "    print(f'=====================\\nValidation Loss: {val_loss.item()}')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    ######## Extracting the weights from the model so we can interpret them ##############\n",
    "    linear_layer = model[0]\n",
    "    weights = linear_layer.weight.data\n",
    "    bias = linear_layer.bias.data\n",
    "    \n",
    "    print(f\"=====================\\nWeights of the model \\n==============\\nYou can interpret this as how much they contribute to each of the Y features, the indexes of the weights corresponds to each of the X features in order, so female percentage, male percentage, asian percentage, black percentage, ... etc are the first index, second index, ...\")\n",
    "    print(f\"\\nAttendance Weights: \\n{weights[0]}\")\n",
    "    print(f\"Chronically Absent Weights: \\n{weights[1]}\")\n",
    "    print(f\"ELA Test Scores: \\n{weights[2]}\")\n",
    "    print(f\"Math Test Scores: \\n{weights[3]}\")\n",
    "    print(f\"=====================\\nTo interpret this, a positive weight for an X feature means that this feature contributes positively to that Y feature. So for my training, Asian Percentage (the third value), had a positive weight for attendance which means that the model learned that a large Asian percentage contributes positively to a larger attendance (this technically isn't linear so we can't say the model learned a linear correlation)\")\n",
    "    print(f\"=====================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0250a4ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_kernel",
   "language": "python",
   "name": "torch_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
